{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1 Word Tokenize.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPGBouQDPNsT",
        "colab_type": "text"
      },
      "source": [
        "# WORD TOKENIZE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bqqmd4HKPQfN",
        "colab_type": "text"
      },
      "source": [
        "Tokenization is the process by which big quantity of text is divided into smaller parts called tokens.\n",
        "\n",
        "These tokens are very useful for finding such patterns as well as is considered as a base step for stemming and lemmatization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVIelsm2PvU8",
        "colab_type": "text"
      },
      "source": [
        "https://keras.io/preprocessing/text/ # Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdC29XMXPsq7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Library\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZUe5BsFP3Tp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = [\n",
        "            'i love my dog',\n",
        "            'I love my cat',\n",
        "            'You love my dog!'\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipLbhIpKP4al",
        "colab_type": "code",
        "outputId": "7e9caf5f-a79f-4e29-a7f4-fa050b51cca3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenizer = Tokenizer(num_words=100) # Initialize tokenizer with max 100 words.\n",
        "\n",
        "tokenizer.fit_on_texts(sentences) # fit on sentence.\n",
        "\n",
        "word_index = tokenizer.word_index # print word_index\n",
        "print(word_index)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6INMK-AzP7PZ",
        "colab_type": "code",
        "outputId": "fa299de1-f0eb-4c95-a85b-1af483a7ee5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Text to sequence\n",
        "sequences = tokenizer.texts_to_sequences(sentences) # tokenize sentences\n",
        "\n",
        "print(sequences)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3, 1, 2, 4], [3, 1, 2, 5], [6, 1, 2, 4]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5iEnuCtQXMd",
        "colab_type": "code",
        "outputId": "48c6c721-149a-43f2-e713-b79c1f92f7de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Test on new data\n",
        "test_data = ['i really love my dog', 'my dog loves my manatee']\n",
        "\n",
        "test_seq = tokenizer.texts_to_sequences(test_data)\n",
        "print(test_seq)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3, 1, 2, 4], [2, 4, 2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0bErKt4QfS2",
        "colab_type": "text"
      },
      "source": [
        "Very no good because new word are not inserted because are unknown.. for this reason, we try to.. specify in tokenizer oov_token = \"<\"OOV\">\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZorcibhQdng",
        "colab_type": "code",
        "outputId": "1896785d-97b1-49d9-af66-4e144ad545fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenizer = Tokenizer(num_words=100, oov_token=\"<00V>\") # add parameter\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<00V>': 1, 'love': 2, 'my': 3, 'i': 4, 'dog': 5, 'cat': 6, 'you': 7}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Heu5P5wSQ5OR",
        "colab_type": "code",
        "outputId": "2ec23108-72d1-4760-b688-6e4ed4932002",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "print(sequences)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[4, 2, 3, 5], [4, 2, 3, 6], [7, 2, 3, 5]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhbNJNWnQ7E6",
        "colab_type": "code",
        "outputId": "34cd7282-2eb9-4600-ad94-44f2d197f011",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_seq = tokenizer.texts_to_sequences(test_data)\n",
        "print(test_seq)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[4, 1, 2, 3, 5], [3, 5, 1, 3, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYQkf8f9RCo5",
        "colab_type": "text"
      },
      "source": [
        "Now where there is a unknow word we put special character OOV"
      ]
    }
  ]
}